{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29bd6020-8be0-4713-aa41-f12c334df884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d56386d1-0184-4b91-93d4-b336b5fb538d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"CShorten/CDC-COVID-FAQ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86ffbc1c-cc17-45e0-9841-7a484988986e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3dd3953-aec8-4f23-9c74-6ede8f77bc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    " \n",
    "    def __init__(self, dataset):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset: A hugging face dataset.\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.max_question_len = 0\n",
    "        self.max_answer_len = 0\n",
    "        self.vocab = self.create_vocab()\n",
    "        self.vocab_len = len(self.vocab)\n",
    "        self.stoi, self.itos = self.create_vocab_dict()\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        urls = re.findall(r'https?://\\S+|www\\.\\S+', text)\n",
    "        for i, url in enumerate(urls):\n",
    "            text = text.replace(url, f'__URL_{i}__')\n",
    "        text = re.sub(r'([,.!?():\\-\\[\\]/])', r' \\1 ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        for i, url in enumerate(urls):\n",
    "            text = text.replace(f'__URL_{i}__', url)\n",
    "        return list(map(str.lower, text.split()))\n",
    "\n",
    "    def create_vocab(self):\n",
    "        word_set = {'<sos>', '<eos>', '<pad>'}\n",
    "        for interaction in self.dataset['train']:\n",
    "            question = interaction['question']\n",
    "            answer = interaction['answer']\n",
    "            question_tokens = self.tokenize(question)\n",
    "            answer_tokens = self.tokenize(answer)\n",
    "            word_set.update(set(question_tokens))\n",
    "            word_set.update(set(answer_tokens))\n",
    "            question_len = len(question_tokens)\n",
    "            answer_len = len(answer_tokens)\n",
    "            self.max_question_len = max(self.max_question_len, question_len)\n",
    "            self.max_answer_len = max(self.max_answer_len, answer_len)\n",
    "        self.max_answer_len += 2\n",
    "        self.max_question_len += 2\n",
    "        return sorted(word_set)\n",
    "        \n",
    "    def create_vocab_dict(self):\n",
    "        stoi = OrderedDict()\n",
    "        itos = OrderedDict()\n",
    "        for idx, word in enumerate(self.vocab):\n",
    "            stoi[word] = idx\n",
    "            itos[idx] = word\n",
    "        return stoi, itos  \n",
    "\n",
    "    def get_data_tensor(self, sample):\n",
    "        question_tensor = torch.zeros(self.max_question_len, self.vocab_len)\n",
    "        question_toks = self.tokenize(sample['question'])\n",
    "        answer_tensor = torch.zeros(self.max_answer_len, self.vocab_len)\n",
    "        answer_toks = self.tokenize('<sos> ' + sample['answer'] + ' <eos>')\n",
    "        question_tensor[:, self.stoi['<pad>']] = 1\n",
    "        answer_tensor[:, self.stoi['<pad>']] = 1\n",
    "        for idx, word in enumerate(question_toks):\n",
    "            question_tensor[idx, self.stoi[word]] = 1\n",
    "            question_tensor[idx, self.stoi['<pad>']] = 0\n",
    "        for idx, word in enumerate(answer_toks):\n",
    "            answer_tensor[idx, self.stoi[word]] = 1\n",
    "            answer_tensor[idx, self.stoi['<pad>']] = 0\n",
    "        return question_tensor, answer_tensor\n",
    "            \n",
    "    def create_data_tensors(self):\n",
    "        questions = torch.zeros(self.dataset.shape['train'][0], self.max_question_len, self.vocab_len)\n",
    "        answers = torch.zeros(self.dataset.shape['train'][0], self.max_answer_len, self.vocab_len)\n",
    "        for i, interaction in enumerate(self.dataset['train']):\n",
    "            question_tensor, answer_tensor = self.get_data_tensor(interaction)\n",
    "            questions[i] = question_tensor\n",
    "            answers[i] = answer_tensor \n",
    "        return questions, answers\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9ea119d-92da-4127-be1a-1ac49b4cd517",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.cell = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=bidirectional, dropout=dropout)\n",
    "\n",
    "    def forward(self, x, device='mps'):\n",
    "        h0, c0 = self.init_hidden_memory(x.shape[0], device=device)\n",
    "        hidden_states, (ht, ct) = self.cell(x, (h0, c0))\n",
    "        ht = torch.permute(ht, (1, 0, 2)).flatten(start_dim=1, end_dim=2).reshape(x.shape[0], self.num_layers, self.num_directions*self.hidden_dim).permute((1, 0, 2))\n",
    "        ct = torch.permute(ht, (1, 0, 2)).flatten(start_dim=1, end_dim=2).reshape(x.shape[0], self.num_layers, self.num_directions*self.hidden_dim).permute((1, 0, 2))\n",
    "        return ht, ct\n",
    "\n",
    "    def init_hidden_memory(self, batch_size, device='mps'):\n",
    "        h0 = torch.zeros(self.num_directions*self.num_layers, batch_size, self.hidden_dim, device=device)\n",
    "        c0 = torch.zeros(self.num_directions*self.num_layers, batch_size, self.hidden_dim, device=device)\n",
    "        return h0, c0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6801a433-ba85-4f00-b6e8-04feedf71f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout, num_classes):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_classes = num_classes\n",
    "        self.cell = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc1 = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x, h0, c0):\n",
    "        outputs, (h0, c0) = self.cell(x, (h0, c0))\n",
    "        outputs = outputs.squeeze(dim=1)\n",
    "        outputs = self.fc1(outputs)\n",
    "        return outputs, (h0, c0)   # returns a (1, vocab_size), (1, 1, hidden_dim), (1, 1, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "606db917-8afa-4103-8bda-63d0361528d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device='mps'\n",
    "tokenizer =Tokenizer(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f02d4c84-62d6-49b2-923c-30845f92309a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, tokenizer):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.loss = nn.CrossEntropyLoss(ignore_index=tokenizer.stoi['<pad>'])\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, source, target, teacher_force_prob=0.4, device='mps'):\n",
    "        xt = target[:, 0:1, :]\n",
    "        ht, ct = self.encoder(source, device=device)\n",
    "        outputs = torch.zeros(source.shape[0], target.shape[1], target.shape[2]).to(device)\n",
    "        loss_at_batch = []\n",
    "        for t in range(1, target.shape[1]):\n",
    "            logits, (ht, ct) = self.decoder(xt, ht, ct)\n",
    "            outputs[:, t, :] = logits\n",
    "            y = target[:, t, :]\n",
    "            y = y.argmax(dim=1).flatten()\n",
    "            loss_at_timestep = self.loss(logits, y)\n",
    "            loss_at_batch.append(loss_at_timestep)\n",
    "            xt = torch.nn.functional.one_hot(logits.argmax(1).flatten(), num_classes=source.shape[-1]).unsqueeze(dim=1).to(torch.float32).to(device) if torch.rand(1) < teacher_force_prob else target[:, t:t+1, :] \n",
    "        return outputs, sum(loss_at_batch)/len(loss_at_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a881d11f-b4a0-45cc-9ddc-d6416cfbe870",
   "metadata": {},
   "outputs": [],
   "source": [
    "device='mps'\n",
    "tokenizer = Tokenizer(df)\n",
    "input_dim = 128\n",
    "encoder = Encoder(tokenizer.vocab_len, input_dim, 1, 0.0, True)\n",
    "decoder = Decoder(tokenizer.vocab_len, 2*input_dim, 1, 0.0, tokenizer.vocab_len)\n",
    "model = Seq2Seq(encoder, decoder, tokenizer)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fb5ff7e-6d31-42ec-b343-c26d979dc4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.samples = x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0dbbe25-2136-4824-a37c-a5ed0f9ca32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_data, answers_data = tokenizer.create_data_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e666233f-9b00-442a-a51c-2f1f1bd23a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Seq2SeqDataset(questions_data, answers_data)\n",
    "data_loader = DataLoader(dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6528d7d7-402f-407e-aef3-466577ce3286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loader, model, training_params):\n",
    "    epochs = training_params['epochs']\n",
    "    optimizer = training_params['optimizer']\n",
    "    min_loss = float('inf')\n",
    "    train_loss_ = None\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = []\n",
    "        for x, y in tqdm(data_loader, desc=f'Epoch: {epoch}/{epochs} train loss = {train_loss_} best_loss = {min_loss}'):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            outputs, loss = model(x, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "        train_loss_ = sum(train_loss)/len(train_loss)\n",
    "        if train_loss_ < min_loss:\n",
    "            torch.save(model.state_dict(), 'Seq2Seq.pth')\n",
    "            min_loss = train_loss_\n",
    "            print('model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55bffbbb-20e2-4cd9-bea6-e9b608148334",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr= 5e-3)\n",
    "training_params = {'optimizer': optimizer, 'epochs': 70}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6e12b78-de50-49f0-bfef-cb92e721c727",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0/70 train loss = None best_loss = inf: 100%|█| 7/7 [00:05<00:00,  1.25it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/70 train loss = 4.211297392845154 best_loss = 4.211297392845154: 100%|█\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2/70 train loss = 3.225475490093231 best_loss = 3.225475490093231: 100%|█\n",
      "Epoch: 3/70 train loss = 3.3849016768591746 best_loss = 3.225475490093231: 100%|\n",
      "Epoch: 4/70 train loss = 3.637118067060198 best_loss = 3.225475490093231: 100%|█\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5/70 train loss = 2.937252002102988 best_loss = 2.937252002102988: 100%|█\n",
      "Epoch: 6/70 train loss = 3.61251357623509 best_loss = 2.937252002102988: 100%|█|\n",
      "Epoch: 7/70 train loss = 3.639039763382503 best_loss = 2.937252002102988: 100%|█\n",
      "Epoch: 8/70 train loss = 3.3392046689987183 best_loss = 2.937252002102988: 100%|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 9/70 train loss = 2.866875103541783 best_loss = 2.866875103541783: 100%|█\n",
      "Epoch: 10/70 train loss = 3.4424594896180287 best_loss = 2.866875103541783: 100%\n",
      "Epoch: 11/70 train loss = 3.4337701967784335 best_loss = 2.866875103541783: 100%\n",
      "Epoch: 12/70 train loss = 3.5782800912857056 best_loss = 2.866875103541783: 100%\n",
      "Epoch: 13/70 train loss = 3.6012931891850064 best_loss = 2.866875103541783: 100%\n",
      "Epoch: 14/70 train loss = 3.28126506294523 best_loss = 2.866875103541783: 100%|█\n",
      "Epoch: 15/70 train loss = 3.7094129834856306 best_loss = 2.866875103541783: 100%\n",
      "Epoch: 16/70 train loss = 3.1529600279671803 best_loss = 2.866875103541783: 100%\n",
      "Epoch: 17/70 train loss = 3.6130632758140564 best_loss = 2.866875103541783: 100%\n",
      "Epoch: 18/70 train loss = 3.4047187737056186 best_loss = 2.866875103541783: 100%\n",
      "Epoch: 19/70 train loss = 3.1955008591924394 best_loss = 2.866875103541783: 100%\n",
      "Epoch: 20/70 train loss = 3.424644180706569 best_loss = 2.866875103541783: 100%|\n",
      "Epoch: 21/70 train loss = 3.0607770681381226 best_loss = 2.866875103541783: 100%\n",
      "Epoch: 22/70 train loss = 3.602907751287733 best_loss = 2.866875103541783: 100%|\n",
      "Epoch: 23/70 train loss = 3.490275604384286 best_loss = 2.866875103541783: 100%|\n",
      "Epoch: 24/70 train loss = 3.0183180442878177 best_loss = 2.866875103541783: 100%\n",
      "Epoch: 25/70 train loss = 3.24396465080125 best_loss = 2.866875103541783: 100%|█\n",
      "Epoch: 26/70 train loss = 3.1674008454595293 best_loss = 2.866875103541783: 100%\n",
      "Epoch: 27/70 train loss = 3.1068839686257497 best_loss = 2.866875103541783: 100%\n",
      "Epoch: 28/70 train loss = 2.9412252051489696 best_loss = 2.866875103541783: 100%\n",
      "Epoch: 29/70 train loss = 3.2676490545272827 best_loss = 2.866875103541783: 100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 30/70 train loss = 2.8519048988819122 best_loss = 2.8519048988819122: 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 31/70 train loss = 2.578582695552281 best_loss = 2.578582695552281: 100%|\n",
      "Epoch: 32/70 train loss = 2.9655002866472517 best_loss = 2.578582695552281: 100%\n",
      "Epoch: 33/70 train loss = 2.98397958278656 best_loss = 2.578582695552281: 100%|█\n",
      "Epoch: 34/70 train loss = 2.712105785097395 best_loss = 2.578582695552281: 100%|\n",
      "Epoch: 35/70 train loss = 2.6555564403533936 best_loss = 2.578582695552281: 100%\n",
      "Epoch: 36/70 train loss = 2.918192437716893 best_loss = 2.578582695552281: 100%|\n",
      "Epoch: 37/70 train loss = 2.8623636024338857 best_loss = 2.578582695552281: 100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 38/70 train loss = 2.4928915670939853 best_loss = 2.4928915670939853: 100\n",
      "Epoch: 39/70 train loss = 2.496480073247637 best_loss = 2.4928915670939853: 100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 40/70 train loss = 2.3885052715029036 best_loss = 2.3885052715029036: 100\n",
      "Epoch: 41/70 train loss = 2.5146006175449918 best_loss = 2.3885052715029036: 100\n",
      "Epoch: 42/70 train loss = 2.547064423561096 best_loss = 2.3885052715029036: 100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 43/70 train loss = 2.07705295085907 best_loss = 2.07705295085907: 100%|█|\n",
      "Epoch: 44/70 train loss = 2.5894304173333302 best_loss = 2.07705295085907: 100%|\n",
      "Epoch: 45/70 train loss = 2.436096029622214 best_loss = 2.07705295085907: 100%|█\n",
      "Epoch: 46/70 train loss = 2.4368966136659895 best_loss = 2.07705295085907: 100%|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 47/70 train loss = 2.0335031620093753 best_loss = 2.0335031620093753: 100\n",
      "Epoch: 48/70 train loss = 2.1890380552836826 best_loss = 2.0335031620093753: 100\n",
      "Epoch: 49/70 train loss = 2.075176315648215 best_loss = 2.0335031620093753: 100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 50/70 train loss = 2.0166260855538503 best_loss = 2.0166260855538503: 100\n",
      "Epoch: 51/70 train loss = 2.0292812160083225 best_loss = 2.0166260855538503: 100\n",
      "Epoch: 52/70 train loss = 2.1982798406055997 best_loss = 2.0166260855538503: 100\n",
      "Epoch: 53/70 train loss = 2.1926471846444264 best_loss = 2.0166260855538503: 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 54/70 train loss = 1.9458290210791997 best_loss = 1.9458290210791997: 100\n",
      "Epoch: 55/70 train loss = 2.0452097143445696 best_loss = 1.9458290210791997: 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 56/70 train loss = 1.7655905442578452 best_loss = 1.7655905442578452: 100\n",
      "Epoch: 57/70 train loss = 1.8193478350128447 best_loss = 1.7655905442578452: 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 58/70 train loss = 1.675076961517334 best_loss = 1.675076961517334: 100%|\n",
      "Epoch: 59/70 train loss = 1.7482873903853553 best_loss = 1.675076961517334: 100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 60/70 train loss = 1.5887049287557602 best_loss = 1.5887049287557602: 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 61/70 train loss = 1.542639387505395 best_loss = 1.542639387505395: 100%|\n",
      "Epoch: 62/70 train loss = 1.656640682901655 best_loss = 1.542639387505395: 100%|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 63/70 train loss = 1.4261757135391235 best_loss = 1.4261757135391235: 100\n",
      "Epoch: 64/70 train loss = 1.6021399753434318 best_loss = 1.4261757135391235: 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 65/70 train loss = 1.400780928986413 best_loss = 1.400780928986413: 100%|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 66/70 train loss = 1.309746380363192 best_loss = 1.309746380363192: 100%|\n",
      "Epoch: 67/70 train loss = 1.4131042616707938 best_loss = 1.309746380363192: 100%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 68/70 train loss = 1.2233220083372933 best_loss = 1.2233220083372933: 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 69/70 train loss = 1.1069410741329193 best_loss = 1.1069410741329193: 100\n"
     ]
    }
   ],
   "source": [
    "train(data_loader, model, training_params) # uncomment to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b55add2d-5cb8-4136-9606-2b27eeb7aa7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('Seq2Seq.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fdcc770-c146-4667-8874-d0683b28744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question, tokenizer, model):\n",
    "    sample = {'question': question, 'answer': ''}\n",
    "    question, sos_token = tokenizer.get_data_tensor(sample)\n",
    "    question = question.unsqueeze(dim=0).to(device)\n",
    "    xt = sos_token[0:1]\n",
    "    xt = xt.unsqueeze(dim=0).to(device)\n",
    "    curr_pred = None\n",
    "    all_preds = []\n",
    "    max_len = 800\n",
    "    ht, ct = model.encoder(question)\n",
    "    with torch.no_grad():\n",
    "        while curr_pred != '<eos>' and len(all_preds) < max_len:\n",
    "            logits, (ht, ct) = model.decoder(xt, ht, ct)\n",
    "            next_word_idx = logits.argmax(dim=1)\n",
    "            xt = torch.nn.functional.one_hot(next_word_idx, num_classes=question.shape[-1]).unsqueeze(dim=1).to(torch.float32).to(device)\n",
    "            curr_pred = tokenizer.itos[next_word_idx.item()]\n",
    "            all_preds.append(curr_pred)\n",
    "    return ' '.join(all_preds[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c5ccf793-a45e-4aa6-a5f9-90dda0c4ef4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = df['train']['question'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "21bd953c-507a-4172-9d36-3019c19d953a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'When should healthcare facilities make changes to interventions based on changes in community transmission levels?'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "be0e0159-334e-4751-9985-697d10f27b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no - touch devices ( ntds ) for sometimes used as healthcare settings as intended to characterize the same time . coinfections with sars - cov - 2 infection . the prior to the existing cleaning and disinfection processes . the patient of transmission - based precautions .'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_answer(question, tokenizer, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641c6391-aa7e-4da4-983d-234bf2149593",
   "metadata": {},
   "source": [
    "<h5>Disadvantages</h5>\n",
    "<ul>\n",
    "<li>Developing Language models using one-hot representation is a bad approach. Models have no idea what each word represents. Using one-hot encoding results in large, sparse input vectors that may not capture semantic relationships between words effectively.\n",
    "</li>\n",
    "<li>\n",
    "    with LSTMs, the models tend to forget longer sequences. Causing these models to be depend greatly on the previous generated word because the hidden states fail to capture the context of question.\n",
    "</li>\n",
    "\n",
    "<li>\n",
    "    The generated text does not make sense for the question given. \n",
    "<li>\n",
    "    The idea of this notebook is to look into what exactly happens in a sequence to sequence model. In general, a validation set is used to find the model which generalizes better. This model is overfitted to the train set. So, it cannot generalize to question which it never saw.\n",
    "</li>\n",
    "<li>In the future, we will implement Embeddings and use the attention module. Embeddings are latent word representations that capture meaningful semantic information. With Attention module, model will be able to capture long range dependencies.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2446ff4e-ab1b-445c-afc0-d456034840d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
